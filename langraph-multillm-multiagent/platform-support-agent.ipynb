{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Platform Support Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class KnowledgeBase(TypedDict):\n",
    "    knowledge: str\n",
    "    source: str\n",
    "\n",
    "class ServiceNowIncident(TypedDict):\n",
    "    title: str\n",
    "    description: str\n",
    "    state: str\n",
    "    id: str\n",
    "    link: str\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    problem: str\n",
    "\n",
    "class ProcessingState(TypedDict):\n",
    "    problem: str\n",
    "    serviceNow: List[ServiceNowIncident]\n",
    "    knowledgeBase: List[KnowledgeBase]\n",
    "    solution: str\n",
    "    search_result: List[str]\n",
    "    errors: List[str]\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    solution: str\n",
    "    source: List[str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "\n",
    "def query_vector_store(question):\n",
    "    urls = [\n",
    "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "        \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "        \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    ]\n",
    "\n",
    "    # Load documents\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=1000, chunk_overlap=200\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "    # Add to vectorDB\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "    )\n",
    "    # Create retriever\n",
    "    retriever = vectorstore.as_retriever(k=3)\n",
    "    docs = retriever.invoke(question)\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain.agents import Tool\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "@tool\n",
    "def get_knowledge(query: str) -> List[KnowledgeBase]:\n",
    "    \"\"\"\n",
    "    Tool call to RAG service will return relevant information about the given keywords\n",
    "    Args:\n",
    "        query: query to get more information for \n",
    "    \"\"\"\n",
    "    try:\n",
    "        return \"Sample knowledge from knowledge server\"\n",
    "    except Exception as e:\n",
    "        return [KnowledgeBase(knowledge=f\"Error accessing knowledge base: {e}\", source=\"\")]\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_knowledge_parallel(keywords: List[str]) -> List[KnowledgeBase]:\n",
    "    \"\"\"Tool call to RAG service will return relevant information about the given keywords\"\"\"\n",
    "    # Implement parallel API calls here\n",
    "    return [{\"knowledge\": \"Sample knowledge from knowledge server\", \"source\": \"www.google.com\"}]\n",
    "\n",
    "\n",
    "@tool\n",
    "def ddg_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    duck_duck_go search tool call with a query about which you want to do web search\n",
    "    Args:\n",
    "        query: query to get more information for\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search = DuckDuckGoSearchRun()\n",
    "        response = search.run(query) # Use .run() instead of .invoke()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error performing DuckDuckGo search: {e}\"\n",
    "\n",
    "\n",
    "tools = [get_knowledge, get_knowledge_parallel]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "search_tools = [ddg_search]\n",
    "search_tool_node = ToolNode(search_tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noepolak1\\AppData\\Local\\Temp\\ipykernel_10320\\3746813217.py:248: LangGraphDeprecationWarning: Initializing StateGraph without state_schema is deprecated. Please pass in an explicit state_schema instead of just an input and output schema.\n",
      "  workflow = StateGraph(input=InputState, output=OutputState)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Define a simple chatbot agent.\n",
    "\n",
    "This agent returns a predefined response without using an actual LLM.\n",
    "\"\"\"\n",
    "\n",
    "from encodings import undefined\n",
    "import json\n",
    "from json import tool\n",
    "from typing import Any, Dict, List, Literal, TypedDict\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pydantic import conset\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI, ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "llm_azure = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    verbose=True\n",
    "    # organization=\"...\",\n",
    "    # model=\"gpt-35-turbo\",\n",
    "    # model_version=\"0125\",\n",
    "    # other params...\n",
    ")\n",
    "#  Initialize VertexAI model\n",
    "llmPro = ChatVertexAI(\n",
    "    model_name=\"gemini-2.0-pro-exp-02-05\",  # You can change to another model as needed\n",
    "    temperature=0,\n",
    "    max_output_tokens=4096,\n",
    "    project=\"sap-genai-playground-dev-mg\",\n",
    "    region=\"us-central1\"\n",
    ")\n",
    "\n",
    "llmFlash = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "\n",
    "llm_with_tool = ChatVertexAI(\n",
    "    model_name=\"gemini-1.5-flash\",  # You can change to another model as needed\n",
    "    temperature=0,\n",
    "    max_output_tokens=4096,\n",
    "    project=\"sap-genai-playground-dev-mg\",\n",
    "    region=\"us-central1\"\n",
    ").bind_tools(tools=tools)\n",
    "\n",
    "llm_with_search = ChatVertexAI(\n",
    "    model_name=\"gemini-1.5-flash\",  # You can change to another model as needed\n",
    "    temperature=0,\n",
    "    max_output_tokens=4096,\n",
    "    project=\"sap-genai-playground-dev-mg\",\n",
    "    region=\"us-central1\"\n",
    ").bind_tools(tools=search_tools)\n",
    "\n",
    "\n",
    "def thinking(inputState: InputState) -> ProcessingState:\n",
    "    \"\"\"Thinking Node which will take user input and spend time reasoning it, analyzing requirements.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are an analyst who has to identify the user's problem, after which you have to share the next steps \"\n",
    "                \"that you need to take to fulfill the user's request. \"\n",
    "                \"<example>\"\n",
    "                \"I need to search the web to get information about <keyword>, I think the user is trying to achieve <this>\"\n",
    "                \"</example>\"\n",
    "                \"Please do not ask for additional information from the user. If there is any additional information required, \"\n",
    "                \"then you have tool_calls which you can use, namely, knowledge_base and web_search. Mention you will have \"\n",
    "                \"to use them if required in your thoughts.\"\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the problem: {prob}, return the output as JSONObject(problem: string)\"\n",
    "        ),\n",
    "    ])\n",
    "    chain =  prompt | llmFlash\n",
    "    try:\n",
    "        # Ensure the input matches the expected format\n",
    "        print(\"Formatted Prompt: \", prompt.format(prob=inputState[\"problem\"]))\n",
    "        # Invoke the chain with the properly formatted input\n",
    "        result = chain.invoke({\"prob\": inputState[\"problem\"]})\n",
    "        print(\"result of thinking: \", result)\n",
    "        if not isinstance(result, AIMessage):\n",
    "            raise ValueError(\"No message found in input\")\n",
    "\n",
    "        return {\n",
    "            \"search_result\": [],\n",
    "            \"problem\": result.content,\n",
    "            \"knowledgeBase\": [],\n",
    "            \"serviceNow\": [],\n",
    "            \"solution\": \"\",\n",
    "            \"errors\": []\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"search_result\": [],\n",
    "            \"problem\": inputState[\"problem\"],\n",
    "            \"knowledgeBase\": [],\n",
    "            \"serviceNow\": [],\n",
    "            \"solution\": \"\",\n",
    "            \"errors\": [f\"Error: {e}\"]\n",
    "        }\n",
    "\n",
    "def should_get_knowledge(processingState: ProcessingState) -> str :\n",
    "    if(processingState[\"knowledgeBase\"] == [] or processingState[\"knowledgeBase\"] == \"\"):\n",
    "        return \"tool_call\"\n",
    "    else:\n",
    "        return \"summarise\"\n",
    "\n",
    "def should_call_service_now(processingState: ProcessingState) -> str :\n",
    "    if(processingState[\"knowledgeBase\"] == []):\n",
    "        return \"summarise\"\n",
    "    else:\n",
    "        return \"service_now\"\n",
    "\n",
    "\n",
    "def knowledge_search(processingState: ProcessingState) -> ProcessingState:\n",
    "    \n",
    "    \"\"\"Will search the Knowledge Base with a query about relevant keywords and get relevant information\"\"\"    \n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "         You are an analyst who have to identify user's problem and return answer as JSONObject(data: string, source: string)[]\n",
    "         \"\"\"),\n",
    "        (\"human\", \"Here are your thoughts on what you want to do: {input_source}\\n Here is the context: {knowledge_base}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llmFlash \n",
    "    \n",
    "    try:\n",
    "        result = chain.invoke({\"input_source\" : processingState[\"problem\"], \"knowledge_base\": processingState[\"knowledgeBase\"]})\n",
    "        print(\"result of input_source: \" , result)\n",
    "        return {\n",
    "            **processingState,\n",
    "            \"knowledgeBase\": result.content,\n",
    "            \"errors\": [],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **processingState,\n",
    "            \"errors\": [f\"Error: {e}\"]\n",
    "        }\n",
    "    \n",
    "\n",
    "\"\"\"Will search the ServiceNOW with a query about relevant keywords and get relevant information\"\"\"\n",
    "def service_now_search(processingState: ProcessingState) -> ProcessingState:\n",
    "    \n",
    "    \"\"\"Will search the ServiceNOW with a query about relevant keywords and get relevant information\"\"\"    \n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        Your job is to return the ServiceNOW Incidents as JSONObject(title: str, description: str, state: str, id: str, link: str)[]\n",
    "        \"\"\"),\n",
    "        (\"human\", \"Here are your thoughts on what you wanted to do: {input_source}\\n Here are the incidents on ServiceNOW: {}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llmFlash | JsonOutputParser()\n",
    "    \n",
    "    try:\n",
    "        result = chain.invoke({\"input_source\" : processingState[\"problem\"], \"incidents\": processingState[\"serviceNow\"]})\n",
    "        print(\"result of input_source: \" , result)\n",
    "        return {\n",
    "            **processingState,\n",
    "            \"knowledgeBase\": result,\n",
    "            \"serviceNow\": [],\n",
    "            \"solution\": \"\",\n",
    "            \"errors\": [],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **processingState,\n",
    "            \"knowledgeBase\": [],\n",
    "            \"serviceNow\": [],\n",
    "            \"solution\": \"\",\n",
    "            \"errors\": [f\"Error: {e}\"]\n",
    "        }\n",
    "    \n",
    "\n",
    "def web_search(processingState: ProcessingState) -> ProcessingState:\n",
    "    \"\"\"\n",
    "    Performs a web search using the duck_duck_go tool.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Invoke the llm_with_search chain with the message\n",
    "        result = llm_with_search.invoke(f\"Gather information that you require based on: {processingState.problem}, based on this return a single line of search query\") \n",
    "\n",
    "        print(\"result of web_search: \", result)  #Check the result\n",
    "\n",
    "        search_result = ddg_search(result.content)\n",
    "\n",
    "        return {\n",
    "            **processingState,\n",
    "            \"search_result\": [search_result]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **processingState,\n",
    "            \"errors\": [f\"Error in web_search: {e}\"],\n",
    "        }\n",
    "\n",
    "def should_search_web(processingState: ProcessingState) -> str :\n",
    "    try:\n",
    "        if(processingState[\"search_result\"] == \"\" or len(processingState[\"search_result\"]) == 0):\n",
    "            return \"duck_duck_go\"\n",
    "        else:\n",
    "            return \"knowledge_base\"\n",
    "    except:\n",
    "        return \"duck_duck_go\"\n",
    "        \n",
    "def summarise(processingState: ProcessingState) -> OutputState:\n",
    "\n",
    "    \"\"\"Will search the Knowledge Base with a query about relevant keywords and get relevant information\"\"\"    \n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        Give all the knowledge_base and search_results from suggest a solution to the user's problem, address the user directly and politely.\n",
    "         \"\"\"),\n",
    "        (\"human\", \"Here was the user's problem for which you thougt to do: {input_source}\\n Here is the knowledge_base: {knowledge_base}\\n give a solution with what steps the user can take in plain and understandable english\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llmPro\n",
    "    \n",
    "    try:\n",
    "        result = chain.invoke({\"input_source\" : processingState[\"problem\"], \"knowledge_base\": processingState[\"knowledgeBase\"]})\n",
    "        print(\"result of summarise: \" , result.content)\n",
    "        return {\n",
    "            \"source\" : [],\n",
    "            \"solution\" : result.content\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"source\" : [],\n",
    "            \"solution\" : f\"Error occurred: {e}\"\n",
    "        }\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(input=InputState, output=OutputState)\n",
    "\n",
    "# Add the node to the graph\n",
    "workflow.add_node(node=\"thinking\",action=thinking)\n",
    "workflow.add_node(node=\"knowledge_base\", action=knowledge_search)\n",
    "workflow.add_node(node=\"tool_call\",action=tool_node)\n",
    "workflow.add_node(node=\"web_search\", action=web_search)\n",
    "workflow.add_node(node=\"duck_duck_go\", action=search_tool_node)\n",
    "workflow.add_node(node=\"summarise\", action=summarise)\n",
    "\n",
    "# Set the entrypoint as `call_model`\n",
    "workflow.set_entry_point(\"thinking\")\n",
    "\n",
    "# Define edges\n",
    "workflow.add_edge(\"__start__\", \"thinking\")\n",
    "workflow.add_edge(\"thinking\", \"web_search\")\n",
    "workflow.add_edge(\n",
    "    \"web_search\",\"knowledge_base\"\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"knowledge_base\"\n",
    "    ,should_get_knowledge\n",
    "    ,[\"tool_call\",\"summarise\"]\n",
    ")\n",
    "workflow.add_edge(\"tool_call\", \"knowledge_base\")\n",
    "workflow.add_edge(\"summarise\", \"__end__\")\n",
    "\n",
    "\n",
    "graph = workflow.compile()\n",
    "graph.name = \"Platform Support Agent\"  # This defines the custom name in LangSmith\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt:  System: You are an analyst who has to identify the user's problem, after which you have to share the next steps that you need to take to fulfill the user's request. <example>I need to search the web to get information about <keyword>, I think the user is trying to achieve <this></example>Please do not ask for additional information from the user. If there is any additional information required, then you have tool_calls which you can use, namely, knowledge_base and web_search. Mention you will have to use them if required in your thoughts.\n",
      "Human: Here is the problem: I am getting an error while making network calls with broken pipe error, return the output as JSONObject(problem: string)\n",
      "result of thinking:  content='I think the user is trying to address an issue related to network calls, specifically encountering a \"broken pipe\" error. The output they want is a JSON object that encapsulates the problem.\\n\\nNext, I will create a JSON object that includes the specified problem. Here\\'s the output:\\n\\n```json\\n{\\n  \"problem\": \"I am getting an error while making network calls with broken pipe error\"\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 148, 'total_tokens': 230, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'finish_reason': 'stop', 'logprobs': None} id='run--4514b722-73d7-4b4f-b557-1c50835cd535-0' usage_metadata={'input_tokens': 148, 'output_tokens': 82, 'total_tokens': 230, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "result of input_source:  content='```json\\n[\\n  {\\n    \"data\": \"I am getting an error while making network calls with broken pipe error\",\\n    \"source\": \"User description of network issue\"\\n  }\\n]\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 137, 'total_tokens': 178, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'finish_reason': 'stop', 'logprobs': None} id='run--cd50087f-80fc-4e34-9199-664b637cfdb6-0' usage_metadata={'input_tokens': 137, 'output_tokens': 41, 'total_tokens': 178, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "\n",
    "    \"problem\": \"I am getting an error while making network calls with broken pipe error\"\n",
    "}\n",
    "result = graph.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Error occurred: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_markdown\n",
    "\n",
    "display_markdown(result[\"solution\"], raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
